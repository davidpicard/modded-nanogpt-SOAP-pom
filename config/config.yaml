defaults:
  - model: d12
  - optimizer: soap
  - data: fineweb
  - _self_
  - experiment: ???

# Training configuration
training:
  batch_size: 48
  accumulation: 2
  sequence_length: 1024
  num_iterations: 750
  learning_rate: 0.002
  warmup_iters: 10
  warmdown_iters: 0
  weight_decay: 0.00001

# Evaluation configuration
evaluation:
  val_loss_every: 128
  val_max_steps: 20
  save_every: 0
  # Text sampling configuration
  sample_every: 512  # How often to generate text samples (defaults to val_loss_every)
  num_unconditional_samples: 5  # Number of unconditional text samples
  num_completion_samples: 5  # Number of completion samples from validation set
  max_new_tokens: 128  # Maximum tokens to generate per sample
  temperature: 1.0  # Sampling temperature
  top_k: 50  # Top-k sampling parameter
  prompt_length: 32  # Length of prompt for completion samples
  sample_seed: 3407  # Seed for reproducible text sampling
  debug_sampling: false  # Print debug info for sampling consistency

# Distributed training
distributed:
  backend: nccl
  find_unused_parameters: false

# Logging
logging:
  log_dir: logs
  log_every: 1

# Hardware
hardware:
  device: cuda
  dtype: bfloat16
  compile: false

experiment_name: baseline